{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "83e92e96c1b2693175187f170e874f80811b915b"
   },
   "source": [
    "# Hands-On-AI-In-Cyber\n",
    "**SMS: Spam or Ham (Beginner)** \n",
    "\n",
    "Credits: https://www.kaggle.com/code/dejavu23/sms-spam-or-ham-beginner/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "14f89fbf45c9e39030ace6d215aa377f75437f69"
   },
   "source": [
    "Ziel: Klassifizierung von SMS texten zu Ham oder Spam\n",
    "\n",
    "Behandelte Themen:\n",
    "\n",
    "* **Removing Punctuation and Stopwords**\n",
    "* **Tokenizer, Bag of words**  \n",
    "* **Term frequency inverse document frequency (TFIDF)**\n",
    "\n",
    "Anhand von diesem Datenset wird ein Naive Bayes model getestet:\n",
    "\n",
    "* **Naive Bayes Classifier**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7e07c8e4ca9941fa0b409bbc48c79f4b432c9832"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d042a0fd2b1b124201c84b367e5a3c811f3c7278"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "a54d45761deee27294673d1aeadebcc1c3eed146"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    mtx = confusion_matrix(y_true, y_pred)\n",
    "    #fig, ax = plt.subplots(figsize=(4,4))\n",
    "    sns.heatmap(mtx, annot=True, fmt='d', linewidths=.5,  \n",
    "                cmap=\"Blues\", cbar=False, ax=ax)\n",
    "    #  square=True,\n",
    "    plt.ylabel('true label')\n",
    "    plt.xlabel('predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "082306c6380a6bc0a2d1aa16843ab4200f011ebe"
   },
   "source": [
    "# **Part 1: Dataset insights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "5654c4b08e286b2833bc03d056878325041c75c9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  spam  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     0     111\n",
       "1   ham                      Ok lar... Joking wif u oni...     0      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     1     155\n",
       "3   ham  U dun say so early hor... U c already then say...     0      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...     0      61"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/spamHamDataSet.csv\",encoding='latin-1')\n",
    "#cleaning data\n",
    "data['spam'] = data['label'].map( {'spam': 1, 'ham': 0} ).astype(int)\n",
    "data['length'] = data['text'].apply(len)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2ce5027974bdd19359abdeeb32dac9c436dcd0f2"
   },
   "source": [
    "### Distribution of the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> \n",
    "    <h3>Aufgabe: Plotte die Verteilung der Labels </h3></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dc9f69c3e24cd2aa2f2559825a7ad4710e361115"
   },
   "source": [
    "# **Part 2: Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "da7836b09691c2ba2a96f9af3bc8662a1fa0d788"
   },
   "source": [
    "**Damit wir das Datenset für klassifizierungs Algorithmen verwenden können muss dieses noch durch einige Schritte durch. Diese beinhalten die folgenden:**\n",
    "* Remove Stop Words (\"uninformative\" Wörter ausschliessen\n",
    "* Tokenization (Jedes Wort wird als \"Token\" angesehen)\n",
    "* Vectorization (Text muss numerisch dargestellt werden)\n",
    "* TF-IDF weighting  (Verhältnis zwischen Wort häufigkeit und Textlänge)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9a89e7cb53fb78632611598e27fcaadb4d00f127"
   },
   "source": [
    "## 2.1 Remove Punctuation and Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b0f0e77fc5ba947aa4585c259938064d464edfbe"
   },
   "source": [
    "### Punctuation\n",
    "**Die folgende punctuation list wird für uns verwendet um den Text zu cleanen:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "76654fd5e10f4c8a46d9d8c9dc5238e0a860d9af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cb9c3c22e0534e2630eed529b3989eac65a302b3"
   },
   "source": [
    "### Stopwords  \n",
    "Sklearn Dokumentation:  https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words  \n",
    "\n",
    "<font color='red'> \n",
    "    <h3>Aufgabe:</h3>\n",
    "    </font>\n",
    "Frage dich, wieso entfernt man stopwords aus Texten?\n",
    "Was sind die Vorteile?\n",
    "Könnte es auch Nachteile haben?\n",
    "\n",
    "\n",
    "<font color='red'>Noch nicht weiter lesen, bis du dir Gedanken gemacht hast!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erklärung\n",
    "Stopwords sind Wörter wie \"und\", \"der\", \"ihn\", die vermutlich keine Informationen zur Darstellung des Inhalts eines Textes liefern und entfernt werden können. Dies vermeidet, dass sie als Signal für Vorhersagen interpretiert werden. (Es gibt usecases, bei denen diese nützlich sein können. Beispiel: Zuordnung von Schreibstilen zu Persönlichkeiten.)\n",
    "\n",
    "Wir verwenden hier die Stopwords der Library NLTK:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b7fde6b90bb4cabe8eba93d07c95a91967062060"
   },
   "source": [
    "**NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\luca7\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "2b71df4ef3915bc73648a5380b29019a461305e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words(\"english\")[100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1815902b628a2ea8c7a65637767a5ca5ee161b1b"
   },
   "source": [
    "**Folgende Funktion entfernt die Stopwords und konvertiert den Text in Kleinschreibweise:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "29f3ba753b6fcd26816110fe86a6995f715ad178"
   },
   "outputs": [],
   "source": [
    "def remove_punctuation_and_stopwords(sms):\n",
    "    \n",
    "    sms_no_punctuation = [ch for ch in sms if ch not in string.punctuation]\n",
    "    sms_no_punctuation = \"\".join(sms_no_punctuation).split()\n",
    "    \n",
    "    sms_no_punctuation_no_stopwords = \\\n",
    "        [word.lower() for word in sms_no_punctuation if word.lower() not in stopwords.words(\"english\")]\n",
    "        \n",
    "    return sms_no_punctuation_no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  spam  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     0     111\n",
       "1   ham                      Ok lar... Joking wif u oni...     0      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     1     155\n",
       "3   ham  U dun say so early hor... U c already then say...     0      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...     0      61"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "fe8cec520d8684c1cd481ea26505999576df8339"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [go, jurong, point, crazy, available, bugis, n...\n",
       "1                       [ok, lar, joking, wif, u, oni]\n",
       "2    [free, entry, 2, wkly, comp, win, fa, cup, fin...\n",
       "3        [u, dun, say, early, hor, u, c, already, say]\n",
       "4    [nah, dont, think, goes, usf, lives, around, t...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'].apply(remove_punctuation_and_stopwords).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  spam  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     0     111\n",
       "1   ham                      Ok lar... Joking wif u oni...     0      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     1     155\n",
       "3   ham  U dun say so early hor... U c already then say...     0      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...     0      61"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "22567bbef309b303857f55083a088c8f38069a50"
   },
   "source": [
    "## 2.2 Bag of words with CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b4f59da6ef1965d82e67a5bcc4784382d393f9f0"
   },
   "source": [
    "### The Bag of Words representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0d3879613c6ea148a9e7b832e9e84e832bf37073"
   },
   "source": [
    "https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction  \n",
    "\n",
    "Die Textanalyse ist ein wichtiger Anwendungsbereich für Algorithmen. Die Rohdaten, eine Folge von Symbolen, können jedoch nicht direkt in die Algorithmen eingespeist werden, da die meisten von ihnen numerische Merkmalsvektoren mit einer festen Größe erwarten und nicht die rohen Textdokumente mit variabler Länge.  \n",
    "Um dieses Problem zu lösen, bietet scikit-learn Hilfsprogramme für die gebräuchlichsten Methoden zur Extraktion von numerischen Merkmalen aus Textinhalten, nämlich:\n",
    "\n",
    "**Tokenization**  \n",
    "Tokenisierung von Zeichenketten und Vergabe einer ganzzahligen ID für jedes mögliche Token, z. B. durch Verwendung von Leerzeichen und Satzzeichen als Tokentrennzeichen.  \n",
    "**Vectorization**  \n",
    "Zählen des Vorkommens von Token in jedem Dokument.\n",
    "**TF-IDF**  \n",
    "Normalisierung und Gewichtung mit abnehmender Bedeutung von Tokens, die in der Mehrzahl der Proben/Dokumente vorkommen. \n",
    "\n",
    "**Bag of Words**  \n",
    "In diesem Schema werden die Merkmale und Stichproben wie folgt definiert:\n",
    "Jede einzelne Token-Häufigkeit (normalisiert oder nicht) wird als Feature behandelt.  \n",
    "Der Vektor aller Tokenhäufigkeiten für ein bestimmtes Dokument wird als multivariate Stichprobe betrachtet.  \n",
    "Ein Korpus von Dokumenten kann somit durch eine Matrix mit einer Zeile pro Dokument und einer Spalte pro Token (z.B. Wort), das im Korpus vorkommt, dargestellt werden.  \n",
    "Wir bezeichnen die Vektorisierung als den allgemeinen Prozess der Umwandlung einer Sammlung von Textdokumenten in numerische Merkmalsvektoren.  \n",
    "\n",
    "Diese spezifische Strategie (Tokenisierung, Zählung und Normalisierung) wird als **Bag of Words** oder \"Bag of n-grams\"-Darstellung bezeichnet. \n",
    "\n",
    "Die Dokumente werden durch das Vorkommen von Wörtern beschrieben, wobei die Informationen über die relative Position der Wörter im Dokument vollständig ignoriert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9c2d71c7f0ce91b0c88c8def0d19f514a9f73fc0"
   },
   "source": [
    "Weitere Informationenen:\n",
    "https://en.wikipedia.org/wiki/Bag-of-words_model  \n",
    "https://en.wikipedia.org/wiki/Document-term_matrix  \n",
    "\n",
    "Einführung zu Bag-of-Words in NLP:  \n",
    "https://medium.com/greyatom/an-introduction-to-bag-of-words-in-nlp-ac967d43b428"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d3ff0bbb14d8550b32f36fc04bddc8cf081d9087"
   },
   "source": [
    "Wir verwenden hier den CountVectorizer from sklearn als BOW (Bag of Words) model.  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html    \n",
    "\n",
    "Als Tokenizer verwenden wir die remove_punctuation_and_stopwords Funktion, die wir oben definiert haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  spam  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     0     111\n",
       "1   ham                      Ok lar... Joking wif u oni...     0      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     1     155\n",
       "3   ham  U dun say so early hor... U c already then say...     0      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...     0      61"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "eda2cf09b93bc33d893d4133515ddef9f3dbf6d3"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_transformer = CountVectorizer(analyzer = remove_punctuation_and_stopwords).fit(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "92ca036b1a95efd0c8fbd336b747a4868a960fcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9432\n"
     ]
    }
   ],
   "source": [
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "069f3f06649b4851e2188aade8bf16611c2ff98b"
   },
   "source": [
    "In allen SMS-Nachrichten zählte bow_transformer 9431 verschiedene Wörter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f78df8555292ca47c4000ea29214044a8a6f193e"
   },
   "source": [
    "### Applying bow_transformer on all messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "7afebe912b171c75db39ff6dd2b2f326bb2ab33e"
   },
   "outputs": [],
   "source": [
    "bow_data = bow_transformer.transform(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "c346737685259d8f2b70a9e5f49b77bbd81892a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 9432)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "e1a8ddbe80a38a8e3b233c769657f4adbc95dcce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49773"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_data.nnz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c050910c4505978b6abf83e638b9efb6b46fc959"
   },
   "source": [
    "**Sparsity: percentage of none zero entries**  \n",
    "https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction  \n",
    "\n",
    "**Sparsity**  \n",
    "Da die meisten Dokumente typischerweise eine sehr kleine Teilmenge der im Korpus verwendeten Wörter verwenden,  \n",
    "wird die resultierende Matrix viele Features haben, die Nullen sind (typischerweise mehr als 99 %).  \n",
    "Eine Sammlung von 10.000 kurzen Textdokumenten (z. B. E-Mails) verwendet beispielsweise ein Vokabular  \n",
    "mit einer Größe in der Größenordnung von insgesamt 100.000 eindeutigen Wörtern, während jedes Dokument 100 bis   \n",
    "1000 eindeutige Wörter verwendet.  \n",
    "Um eine solche Matrix im Speicher speichern zu können, aber auch um algebraische Operationen zu beschleunigen, verwenden Matrix/Vektor,  \n",
    "werden Implementierungen typischerweise eine spärliche Darstellung verwenden, wie sie im scipy.sparse-Paket verfügbar ist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "946ea9e4f865aba3e6d28da6b326c6e0b7b3dd0d"
   },
   "source": [
    "Anzahl nicht Null Elemente geteilt durch die Matrixgrösse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "de2aee91877b45651de69cdf576ee79a5bd6c04a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09470631054216923\n"
     ]
    }
   ],
   "source": [
    "print( bow_data.nnz / (bow_data.shape[0] * bow_data.shape[1]) *100 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c58e4914d2bfc63fe69c94e1e02a8db14378cb6b"
   },
   "source": [
    "Ungefähr 10% der Matrix sind nicht Null (=ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8968ae791a11a649e589b95236f8515d12d0dba1"
   },
   "source": [
    "## 2.3 Term frequency inverse document frequency - TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cecafb740fb49406220b1c354ce9a429e822e1e7"
   },
   "source": [
    "### From occurrences to frequencies  \n",
    "https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#from-occurrences-to-frequencies\n",
    "\n",
    "Die Anzahl der Vorkommnisse ist ein guter Anfang, aber es gibt ein Problem: Längere Dokumente haben im Durchschnitt höhere Zählwerte als kürzere Dokumente, auch wenn sie über dieselben Themen sprechen.  \n",
    "Um diese potenziellen Diskrepanzen zu vermeiden, reicht es aus, die Anzahl der Vorkommen jedes Worts in einem Dokument durch die Gesamtzahl der Wörter im Dokument zu teilen: Diese neuen Merkmale werden **tf für Term Frequencies** genannt.  \n",
    "\n",
    "Eine weitere Verfeinerung von tf ist die Herabstufung der Gewichte für Wörter, die in vielen Dokumenten im Korpus vorkommen und daher weniger informativ sind als solche, die nur in einem Dokument vorkommen.  \n",
    "daher weniger informativ sind als diejenigen, die nur in einem kleineren Teil des Korpus vorkommen.  \n",
    "Diese Herabstufung wird **tf-idf für \"Term Frequency times Inverse Document Frequency \"** genannt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e9d280f9fcc8a25e59e1dca52d5de26dc3ba3de1"
   },
   "source": [
    "Mehr Informationen:\n",
    "https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c3f25088775f0caa568fef7164a0475d27f5302a"
   },
   "source": [
    "https://www.kaggle.com/adamschroeder/countvectorizer-tfidfvectorizer-predict-comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "574b03a52794e4124d47e653f44320e29f73a169"
   },
   "source": [
    "### TfidfTransformer from sklearn\n",
    "Sowohl tf als auch tf-idf können mit TfidfTransformer wie folgt berechnet werden:   \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "bc362886dcc6db5f68e03031d87dece50b0b500f"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer().fit(bow_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "71d9302f4d0895922b5d7197338bf482410279e2"
   },
   "outputs": [],
   "source": [
    "data_tfidf = tfidf_transformer.transform(bow_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "46472347f3522d5669bf6dd48fd759570b079378"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5572x9432 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 49773 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 9432)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nur TFIDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_tfidf_train, data_tfidf_test, label_train, label_test = \\\n",
    "    train_test_split(data_tfidf, data[\"spam\"], test_size=0.3, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für TFIDF matrix and feature \"length\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import  hstack\n",
    "X2 = hstack((data_tfidf ,np.array(data['length'])[:,None])).A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train, X2_test, y2_train, y2_test = \\\n",
    "    train_test_split(X2, data[\"spam\"], test_size=0.3, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f206a0ea10a5997e7509bccd25e2d72e045f56ae"
   },
   "source": [
    "# Part 3: Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sparse matrix to matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tfidf_train = data_tfidf_train.A\n",
    "data_tfidf_test = data_tfidf_test.A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNB Model mit TFIDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.965311004784689\n"
     ]
    }
   ],
   "source": [
    "spam_detect_model = MultinomialNB().fit(data_tfidf_train, label_train)\n",
    "pred_test_MNB = spam_detect_model.predict(data_tfidf_test)\n",
    "acc_MNB = accuracy_score(label_test, pred_test_MNB)\n",
    "print(acc_MNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6cbc3816802cc4cfe056dd7d6c906955b84526fd"
   },
   "source": [
    "Erster Classifier funktioniert schon nicht schlecht: \n",
    "Er hat eine Accuracy von 96.5 % für das Testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data_tfidf_train_sc = scaler.fit_transform(data_tfidf_train)\n",
    "data_tfidf_test_sc  = scaler.transform(data_tfidf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNB Model mit TFIDF matrix, aber die Werte sind gescaled mit MinMaxScaler (Alles zwischen 0 und 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9826555023923444\n"
     ]
    }
   ],
   "source": [
    "spam_detect_model_minmax = MultinomialNB().fit(data_tfidf_train_sc, label_train)\n",
    "pred_test_MNB = spam_detect_model_minmax.predict(data_tfidf_test_sc)\n",
    "acc_MNB = accuracy_score(label_test, pred_test_MNB)\n",
    "print(acc_MNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Anwendung des Min-Max-Skalierers auf die TFIDF-Matrix verbessert die Leistung des MNB-Classifiers:  \n",
    "Er hat nun eine Accuracy von 98,2 % für das Testset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3900, 9432)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tfidf_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'><h1>Aufgabe:</h1></font>\n",
    "### Kannst du ein Neuronales Netzwerk erstellen, welches die Texte klassifiziert?\n",
    "### Findest du eine bessere Lösung?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kannst du deine eigene Texte klassifizieren lassen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
