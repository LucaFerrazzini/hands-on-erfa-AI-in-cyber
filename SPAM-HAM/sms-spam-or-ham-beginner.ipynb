{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3cac5ebe5effbc8c22ced3bd85a3d3e827104cf6"
   },
   "source": [
    "![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQUXi3mkDlIZMmaGJzZVQnEEC535eNtp3WbO5HzZMxhCcUwucLo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "83e92e96c1b2693175187f170e874f80811b915b"
   },
   "source": [
    "# **SMS: Spam or Ham (Beginner)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "14f89fbf45c9e39030ace6d215aa377f75437f69"
   },
   "source": [
    "For my first kernel on Natural Language Processing (NLP), I chose the SMS Spam Collection Dataset.  \n",
    "It contains  the text of 5572 SMS messages and a label, classifying the message as \"spam\" or \"ham\".\n",
    "\n",
    "In this kernel I explore some common techniques of NLP like:\n",
    "\n",
    "* **Removing Punctuation and Stopwords**\n",
    "* **Tokenizer, Bag of words**  \n",
    "* **Term frequency inverse document frequency (TFIDF)**\n",
    "\n",
    "Based on these preprocessing, I train 6 different models that classify **unknown** messages as spam or ham. \n",
    "\n",
    "* **Naive Bayes Classifier**\n",
    "* **SVM Classifier**  \n",
    "* **KNN Classifier**\n",
    "* **SGD Classifier**\n",
    "* **Gradient Boosting Classifier**\n",
    "* **XGBoost Classifier**\n",
    "\n",
    "For easier handling of the preprocessing steps (for train and test data) and the optimization of different  \n",
    "models for the same conditions, the classification is done with **Pipelines** including GridSearchCV.  \n",
    "Finally, for the model evaluation different **metrics** are examined:  \n",
    "accuracy, precision, recall, fscore, roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d31e3961805d4b1205440831bacd4796a3d5e281"
   },
   "source": [
    "# **Part 0: Imports, define functions** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7e07c8e4ca9941fa0b409bbc48c79f4b432c9832"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import wordcloud\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d042a0fd2b1b124201c84b367e5a3c811f3c7278"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "a54d45761deee27294673d1aeadebcc1c3eed146"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    mtx = confusion_matrix(y_true, y_pred)\n",
    "    #fig, ax = plt.subplots(figsize=(4,4))\n",
    "    sns.heatmap(mtx, annot=True, fmt='d', linewidths=.5,  \n",
    "                cmap=\"Blues\", cbar=False, ax=ax)\n",
    "    #  square=True,\n",
    "    plt.ylabel('true label')\n",
    "    plt.xlabel('predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "082306c6380a6bc0a2d1aa16843ab4200f011ebe"
   },
   "source": [
    "# **Part 1: EDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "5654c4b08e286b2833bc03d056878325041c75c9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  spam  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     0     111\n",
       "1   ham                      Ok lar... Joking wif u oni...     0      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     1     155\n",
       "3   ham  U dun say so early hor... U c already then say...     0      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...     0      61"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/spamHamDataSet.csv\",encoding='latin-1')\n",
    "#cleaning data\n",
    "data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "data = data.rename(columns={\"v1\":\"label\", \"v2\":\"text\"})\n",
    "data['spam'] = data['label'].map( {'spam': 1, 'ham': 0} ).astype(int)\n",
    "data['length'] = data['text'].apply(len)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2ce5027974bdd19359abdeeb32dac9c436dcd0f2"
   },
   "source": [
    "### Distribution of the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "6a4c90bd39e5bb963d845562e66854455b9118ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.label.value_counts().plot.bar(figsize=(8,4))\n",
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dc9f69c3e24cd2aa2f2559825a7ad4710e361115"
   },
   "source": [
    "# **Part 2: Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "da7836b09691c2ba2a96f9af3bc8662a1fa0d788"
   },
   "source": [
    "**Basic preprocessing for common NLP tasks includes converting text to lowercase and removing punctuation and stopwords.**  \n",
    "**Further steps, especially for text classification tasks, are:**  \n",
    "* Tokenization\n",
    "* Vectorization and \n",
    "* TF-IDF weighting  \n",
    "\n",
    "**Lets apply these approaches on the SMS messages.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9a89e7cb53fb78632611598e27fcaadb4d00f127"
   },
   "source": [
    "## 2.1 Remove Punctuation and Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b0f0e77fc5ba947aa4585c259938064d464edfbe"
   },
   "source": [
    "### Punctuation\n",
    "**We use the punctuation list from the string library:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "76654fd5e10f4c8a46d9d8c9dc5238e0a860d9af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cb9c3c22e0534e2630eed529b3989eac65a302b3"
   },
   "source": [
    "### Stopwords  \n",
    "from sklearn documentation:  https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words  \n",
    "Stop words are words like “and”, “the”, “him”, which are presumed to be uninformative in representing the content of a text,   \n",
    "and which may be removed to avoid them being construed as signal for prediction.  \n",
    "Sometimes, however, similar words are useful for prediction, such as in classifying writing style or personality.  \n",
    "\n",
    "Due to the known issues in the ’english’ stop word list of sklearn, we use the stopwords from NLTK:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b7fde6b90bb4cabe8eba93d07c95a91967062060"
   },
   "source": [
    "**NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/LFE/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "2b71df4ef3915bc73648a5380b29019a461305e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words(\"english\")[100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1815902b628a2ea8c7a65637767a5ca5ee161b1b"
   },
   "source": [
    "**With the above lists for punctuation characters and stop words, we define a function to remove these from the text**  \n",
    "**This function also converts all text to lowercase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "29f3ba753b6fcd26816110fe86a6995f715ad178"
   },
   "outputs": [],
   "source": [
    "def remove_punctuation_and_stopwords(sms):\n",
    "    \n",
    "    sms_no_punctuation = [ch for ch in sms if ch not in string.punctuation]\n",
    "    sms_no_punctuation = \"\".join(sms_no_punctuation).split()\n",
    "    \n",
    "    sms_no_punctuation_no_stopwords = \\\n",
    "        [word.lower() for word in sms_no_punctuation if word.lower() not in stopwords.words(\"english\")]\n",
    "        \n",
    "    return sms_no_punctuation_no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  spam  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     0     111\n",
       "1   ham                      Ok lar... Joking wif u oni...     0      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     1     155\n",
       "3   ham  U dun say so early hor... U c already then say...     0      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...     0      61"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "fe8cec520d8684c1cd481ea26505999576df8339"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [go, jurong, point, crazy, available, bugis, n...\n",
       "1                       [ok, lar, joking, wif, u, oni]\n",
       "2    [free, entry, 2, wkly, comp, win, fa, cup, fin...\n",
       "3        [u, dun, say, early, hor, u, c, already, say]\n",
       "4    [nah, dont, think, goes, usf, lives, around, t...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'].apply(remove_punctuation_and_stopwords).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  spam  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     0     111\n",
       "1   ham                      Ok lar... Joking wif u oni...     0      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     1     155\n",
       "3   ham  U dun say so early hor... U c already then say...     0      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...     0      61"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "22567bbef309b303857f55083a088c8f38069a50"
   },
   "source": [
    "## 2.2 Bag of words with CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b4f59da6ef1965d82e67a5bcc4784382d393f9f0"
   },
   "source": [
    "### The Bag of Words representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0d3879613c6ea148a9e7b832e9e84e832bf37073"
   },
   "source": [
    "https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction  \n",
    "\n",
    "Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.  \n",
    "In order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:\n",
    "\n",
    "**Tokenization**  \n",
    "tokenizing strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.  \n",
    "**Vectorization**  \n",
    "counting the occurrences of tokens in each document.  \n",
    "**TF-IDF**  \n",
    "normalizing and weighting with diminishing importance tokens that occur in the majority of samples / documents.  \n",
    "\n",
    "\n",
    "**Bag of Words**  \n",
    "In this scheme, features and samples are defined as follows:\n",
    "each individual token occurrence frequency (normalized or not) is treated as a feature.  \n",
    "the vector of all the token frequencies for a given document is considered a multivariate sample.  \n",
    "A corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus.  \n",
    "We call vectorization the general process of turning a collection of text documents into numerical feature vectors.   \n",
    "This specific strategy (tokenization, counting and normalization) is called the **Bag of Words** or “Bag of n-grams” representation.  \n",
    "Documents are described by word occurrences while completely ignoring the relative position information of the words in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9c2d71c7f0ce91b0c88c8def0d19f514a9f73fc0"
   },
   "source": [
    "For futher details and example implementations see:  \n",
    "https://en.wikipedia.org/wiki/Bag-of-words_model  \n",
    "https://en.wikipedia.org/wiki/Document-term_matrix  \n",
    "\n",
    "An Introduction to Bag-of-Words in NLP  \n",
    "https://medium.com/greyatom/an-introduction-to-bag-of-words-in-nlp-ac967d43b428"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d3ff0bbb14d8550b32f36fc04bddc8cf081d9087"
   },
   "source": [
    "In this kernel we apply the CountVectorizer from sklearn as BOW model.  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html    \n",
    "As tokenizer we use the remove_punctuation_and_stopwords function defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  spam  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     0     111\n",
       "1   ham                      Ok lar... Joking wif u oni...     0      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     1     155\n",
       "3   ham  U dun say so early hor... U c already then say...     0      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...     0      61"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "eda2cf09b93bc33d893d4133515ddef9f3dbf6d3"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_transformer = CountVectorizer(analyzer = remove_punctuation_and_stopwords).fit(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "92ca036b1a95efd0c8fbd336b747a4868a960fcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9431\n"
     ]
    }
   ],
   "source": [
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "069f3f06649b4851e2188aade8bf16611c2ff98b"
   },
   "source": [
    "In all sms messages bow_transformer counted 9431 different words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f78df8555292ca47c4000ea29214044a8a6f193e"
   },
   "source": [
    "### Applying bow_transformer on all messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "7afebe912b171c75db39ff6dd2b2f326bb2ab33e"
   },
   "outputs": [],
   "source": [
    "bow_data = bow_transformer.transform(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "c346737685259d8f2b70a9e5f49b77bbd81892a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 9431)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "e1a8ddbe80a38a8e3b233c769657f4adbc95dcce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49772"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_data.nnz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c050910c4505978b6abf83e638b9efb6b46fc959"
   },
   "source": [
    "**Sparsity: percentage of none zero entries**  \n",
    "https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction  \n",
    "**Sparsity**  \n",
    "As most documents will typically use a very small subset of the words used in the corpus,  \n",
    "the resulting matrix will have many feature values that are zeros (typically more than 99% of them).  \n",
    "For instance a collection of 10,000 short text documents (such as emails) will use a vocabulary  \n",
    "with a size in the order of 100,000 unique words in total while each document will use 100 to   \n",
    "1000 unique words individually.  \n",
    "In order to be able to store such a matrix in memory but also to speed up algebraic operations matrix / vector,  \n",
    "implementations will typically use a sparse representation such as available in the scipy.sparse package.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "946ea9e4f865aba3e6d28da6b326c6e0b7b3dd0d"
   },
   "source": [
    "number of none zero entries divided by matrix size  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "de2aee91877b45651de69cdf576ee79a5bd6c04a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09471444959776236\n"
     ]
    }
   ],
   "source": [
    "print( bow_data.nnz / (bow_data.shape[0] * bow_data.shape[1]) *100 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c58e4914d2bfc63fe69c94e1e02a8db14378cb6b"
   },
   "source": [
    "Around 10% of the matrix are non zeros (=ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8968ae791a11a649e589b95236f8515d12d0dba1"
   },
   "source": [
    "## 2.3 Term frequency inverse document frequency - TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cecafb740fb49406220b1c354ce9a429e822e1e7"
   },
   "source": [
    "### From occurrences to frequencies  \n",
    "https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#from-occurrences-to-frequencies\n",
    "\n",
    "Occurrence count is a good start but there is an issue: longer documents will have higher average count values  \n",
    "than shorter documents, even though they might talk about the same topics.  \n",
    "To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document  \n",
    "by the total number of words in the document: these new features are called **tf for Term Frequencies**.  \n",
    "Another refinement on top of tf is to downscale weights for words that occur in many documents in the corpus and are  \n",
    "therefore less informative than those that occur only in a smaller portion of the corpus.  \n",
    "This downscaling is called **tf–idf for “Term Frequency times Inverse Document Frequency”**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e9d280f9fcc8a25e59e1dca52d5de26dc3ba3de1"
   },
   "source": [
    "For futher details and example implementations see:  \n",
    "https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c3f25088775f0caa568fef7164a0475d27f5302a"
   },
   "source": [
    "https://www.kaggle.com/adamschroeder/countvectorizer-tfidfvectorizer-predict-comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "574b03a52794e4124d47e653f44320e29f73a169"
   },
   "source": [
    "### TfidfTransformer from sklearn\n",
    "Both tf and tf–idf can be computed as follows using TfidfTransformer:   \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "bc362886dcc6db5f68e03031d87dece50b0b500f"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer().fit(bow_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "71d9302f4d0895922b5d7197338bf482410279e2"
   },
   "outputs": [],
   "source": [
    "data_tfidf = tfidf_transformer.transform(bow_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "46472347f3522d5669bf6dd48fd759570b079378"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5572x9431 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 49772 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 9431)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(data_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for TFIDF matrix only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_tfidf_train, data_tfidf_test, label_train, label_test = \\\n",
    "    train_test_split(data_tfidf, data[\"spam\"], test_size=0.3, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3900x9431 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 35125 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tfidf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "c060c8eb5dcc435b48c5b767a89a17c962893295"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1672x9431 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 14647 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tfidf_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for TFIDF matrix and feature \"length\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import  hstack\n",
    "X2 = hstack((data_tfidf ,np.array(data['length'])[:,None])).A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train, X2_test, y2_train, y2_test = \\\n",
    "    train_test_split(X2, data[\"spam\"], test_size=0.3, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f206a0ea10a5997e7509bccd25e2d72e045f56ae"
   },
   "source": [
    "# Part 3: Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2c4181849789efae863fee973649f7827f2bd20e"
   },
   "source": [
    "## 3.1 First test for Classification  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sparse matrix to matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tfidf_train = data_tfidf_train.A\n",
    "data_tfidf_test = data_tfidf_test.A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNB Model using only TFIDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.965311004784689\n"
     ]
    }
   ],
   "source": [
    "spam_detect_model = MultinomialNB().fit(data_tfidf_train, label_train)\n",
    "pred_test_MNB = spam_detect_model.predict(data_tfidf_test)\n",
    "acc_MNB = accuracy_score(label_test, pred_test_MNB)\n",
    "print(acc_MNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6cbc3816802cc4cfe056dd7d6c906955b84526fd"
   },
   "source": [
    "Our first classifier seems to work well, it has an accuracy of 96.5 % for the test set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data_tfidf_train_sc = scaler.fit_transform(data_tfidf_train)\n",
    "data_tfidf_test_sc  = scaler.transform(data_tfidf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNB Model using only TFIDF matrix, scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9826555023923444\n"
     ]
    }
   ],
   "source": [
    "spam_detect_model_minmax = MultinomialNB().fit(data_tfidf_train_sc, label_train)\n",
    "pred_test_MNB = spam_detect_model_minmax.predict(data_tfidf_test_sc)\n",
    "acc_MNB = accuracy_score(label_test, pred_test_MNB)\n",
    "print(acc_MNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the min max scaler on the TFIDF matrix improves the performance of the MNB classifier:  \n",
    "It now has an accuracy of 98.2 % for the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNB model with TFIDF matrix and feature \"length\", unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8761961722488039\n"
     ]
    }
   ],
   "source": [
    "spam_detect_model_2 = MultinomialNB().fit(X2_train, y2_train)\n",
    "pred_test_MNB_2 = spam_detect_model_2.predict(X2_test)\n",
    "acc_MNB_2 = accuracy_score(y2_test, pred_test_MNB_2)\n",
    "print(acc_MNB_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting MNB with the unscaled features TFIDF + length of message decreases performance.  \n",
    "Lets now check the fit with the scaled features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_tfidf_train = X2_train[:,0:9431]\n",
    "X2_tfidf_test  = X2_test[:,0:9431]\n",
    "X2_length_train = X2_train[:,9431]\n",
    "X2_length_test  = X2_test[:,9431]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X2_tfidf_train = scaler.fit_transform(X2_tfidf_train)\n",
    "X2_tfidf_test  = scaler.transform(X2_tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X2_length_train = scaler.fit_transform(X2_length_train.reshape(-1, 1))\n",
    "X2_length_test  = scaler.transform(X2_length_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train = np.hstack((X2_tfidf_train, X2_length_train))\n",
    "X2_test  = np.hstack((X2_tfidf_test,  X2_length_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNB model with TFIDF matrix and feature \"length\", scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9826555023923444\n"
     ]
    }
   ],
   "source": [
    "spam_detect_model_3 = MultinomialNB().fit(X2_train, y2_train)\n",
    "pred_test_MNB_3 = spam_detect_model_3.predict(X2_test)\n",
    "acc_MNB_3 = accuracy_score(y2_test, pred_test_MNB_3)\n",
    "print(acc_MNB_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We studied the same classifier, Multinomial Naive Bayes, with different set of features and found that the results vary regarding the accuracy of the predictions.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
